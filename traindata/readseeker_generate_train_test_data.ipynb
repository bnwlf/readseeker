{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import gzip\n",
    "import multiprocessing\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm.notebook import tqdm as tqdmn\n",
    "\n",
    "def make_folder_if_required(path):\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepathSummary = \"240325_reference_genomes/gbff\"\n",
    "datasets = [\"viral\",\"bacteria\"]\n",
    "summaryURL = \"https://ftp.ncbi.nlm.nih.gov/genomes/refseq/{dataset}/assembly_summary.txt\"\n",
    "fastaOut = \"240325_reference_genomes/fasta\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files2download = {d:set() for d in datasets}\n",
    "\n",
    "## Manually add Mammalian Reference Genomes\n",
    "files2download.update({\n",
    "    \"mammal/pig\":  [\"http://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Sus_scrofa/latest_assembly_versions/GCF_000003025.6_Sscrofa11.1/GCF_000003025.6_Sscrofa11.1_genomic.gbff.gz\"],\n",
    "    \"mammal/human\":{\"http://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/latest_assembly_versions/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.gbff.gz\"},\n",
    "    \"mammal/bat\":{\"http://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Rhinolophus_ferrumequinum/latest_assembly_versions/GCF_004115265.2_mRhiFer1_v1.p/GCF_004115265.2_mRhiFer1_v1.p_genomic.gbff.gz\"}\n",
    "          })\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    url = summaryURL.format(dataset=dataset)\n",
    "    ncbiresponse = requests.get(url)\n",
    "    make_folder_if_required(os.path.join(basepathSummary,dataset))\n",
    "    with gzip.open (os.path.join(basepathSummary,dataset+\"_assembly_summary.txt.gz\"),'wt') as summarygz:\n",
    "        summarygz.write(ncbiresponse.text)\n",
    "    with gzip.open(os.path.join(basepathSummary,dataset+\"_assembly_summary_filtered.txt.gz\"),'wt') as summaryFgz:\n",
    "        lines = ncbiresponse.text.split(\"\\n\")\n",
    "        lines[1] = lines[1].replace('# ','') # Fix tsv header\n",
    "        #filtered csv-writer\n",
    "        reader = csv.DictReader(lines[1:],delimiter=str(\"\\t\"))\n",
    "        writer = csv.DictWriter(summaryFgz, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            if row.get(\"genome_rep\")==\"Full\" and row.get(\"assembly_level\")==\"Complete Genome\":\n",
    "                downloadpath = row[\"ftp_path\"]+\"/\"+row[\"ftp_path\"].split(\"/\")[-1]+\"_genomic.gbff.gz\"\n",
    "                downloadpath=downloadpath.replace(\"https://\",\"http://\")\n",
    "                files2download[dataset].add(downloadpath)\n",
    "                writer.writerow(row)\n",
    "\n",
    "for key, s in files2download.items(): \n",
    "    print(key,\"-\", len(s))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902b085",
   "metadata": {},
   "source": [
    "# Download gbff.gz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadGBFFgz(url,typ):\n",
    "    \n",
    "    filepath = os.path.join(basepathSummary,typ,url.split(\"/\")[-1])\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        make_folder_if_required(os.path.join(basepathSummary,typ))\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        if r.status_code!=200:\n",
    "            print (r.status_code, url, typ)\n",
    "            return (r.status_code, url, typ)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(url,typ)\n",
    "        print(\"____\")\n",
    "        return(e,url,typ)\n",
    "        \n",
    "\n",
    "def star_downloadGBFFgz(args):\n",
    "    return downloadGBFFgz(*args)\n",
    "                \n",
    "\n",
    "fileTup = [(url,typ) for typ, allurls in files2download.items() for url in allurls ]\n",
    "with multiprocessing.Pool(processes=5) as pool:\n",
    "    failed ={x for x in tqdmn(pool.imap(star_downloadGBFFgz,fileTup),total=len(fileTup)) if x }\n",
    "    for x in failed:\n",
    "        print(\"Failed:\",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(failed))\n",
    "for x in failed:\n",
    "    print(\"wget\", x[1],\"-P\" ,x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b6ec2",
   "metadata": {},
   "source": [
    "# Make indexed Genomic Regions form gbff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20eeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CODINGREGION:\n",
    "    def __init__(self,refid,locustags,start,stop):\n",
    "        self.refid = refid\n",
    "        self.locustags = locustags\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "    def __ge__(self, other):\n",
    "        assert self.refid == other.refid\n",
    "        return self.stop >= other.stop\n",
    "    def __gt__(self, other):\n",
    "        assert self.refid == other.refid\n",
    "        return self.stop > other.stop\n",
    "    def __iadd__(self,other):\n",
    "        assert (other in self)\n",
    "        assert self.refid == other.refid\n",
    "        self.start = min(self.start,other.start)\n",
    "        self.stop = max(self.stop,other.stop)\n",
    "        self.locustags += other.locustags\n",
    "        return self\n",
    "        \n",
    "    def __contains__(self,other):\n",
    "        return self.start<=other.start<=self.stop or self.start<=other.stop<=self.stop\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CODINGREGION({self.refid},{self.locustags},{self.start},{self.stop})\"\n",
    "    \n",
    "class GBFF_ENTRY:\n",
    "    def __init__(self,entry):\n",
    "        self.entry = entry\n",
    "        self.seqid =f'{entry.annotations.get(\"accessions\")[0]}.{entry.annotations.get(\"sequence_version\")}'\n",
    "        self.totallength = len(entry.seq)\n",
    "        self.codingObjList = []\n",
    "        self.taxid  = -1\n",
    "        self.singeleFeatures = []\n",
    "        for feature in entry.features:\n",
    "            if feature.type == \"source\":\n",
    "                taxid = [x[6:] for x in entry.features[0].qualifiers.get(\"db_xref\",[]) if x.startswith(\"taxon:\")][0]\n",
    "                if taxid:\n",
    "                    self.taxid = taxid\n",
    "                \n",
    "            if feature.type == \"CDS\":\n",
    "                assert self.taxid != -1\n",
    "                for seqElem in feature.location.parts:\n",
    "                    locustags = feature.qualifiers['locus_tag'] if 'locus_tag' in feature.qualifiers else []\n",
    "                    self.singeleFeatures.append(CODINGREGION(self.seqid,locustags,seqElem.nofuzzy_start,seqElem.nofuzzy_end))\n",
    "        #Merge Features if overlapping\n",
    "        self.singeleFeatures.sort()\n",
    "        \n",
    "        for x in self.singeleFeatures:\n",
    "            if len(self.codingObjList) and (x in self.codingObjList[-1]):\n",
    "                self.codingObjList[-1]+=x\n",
    "            else:\n",
    "                self.codingObjList.append(x)\n",
    "\n",
    "    \n",
    "    def get_coding(self,minsize = 300):\n",
    "        return [(\n",
    "            self.seqid,\n",
    "            x.start,\n",
    "            x.stop,\n",
    "            \"__\".join(x.locustags),\n",
    "            str(self.entry.seq[x.start:x.stop]),\n",
    "            self.taxid\n",
    "        ) for x in self.codingObjList  if (x.stop-x.start)>= minsize]\n",
    "        \n",
    "    \n",
    "    def get_noncoding(self,minsize = 300):\n",
    "        noncoding=[]\n",
    "        # Add starting piece as non-coding, if starting piece >= minsize\n",
    "        if self.codingObjList and minsize<=self.codingObjList[0].start:\n",
    "           \n",
    "            noncoding.append(\n",
    "                (self.seqid,\n",
    "                 0,\n",
    "                 self.codingObjList[0].start,\n",
    "                 f\"{self.seqid}_noncoding_0\",\n",
    "                 str(self.entry.seq[0:self.codingObjList[0].start]),\n",
    "                 self.taxid\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Add middle pieces between cds as noncoding\n",
    "        if len(self.codingObjList)>1:\n",
    "            noncoding+= [\n",
    "                (self.seqid,\n",
    "                 self.codingObjList[i-1].stop,\n",
    "                 self.codingObjList[i].start,\n",
    "                 f\"{self.seqid}_noncoding_{i}\",\n",
    "                 str(self.entry.seq[self.codingObjList[i-1].stop:self.codingObjList[i].start]),\n",
    "                 self.taxid\n",
    "                )\n",
    "                \n",
    "                for i in range(1,len(self.codingObjList)) if (self.codingObjList[i].start-self.codingObjList[i-1].stop)>=minsize]\n",
    "        \n",
    "        \n",
    "        # Add Endpiece as non-coding, if Endpiece >= minsize\n",
    "        if self.codingObjList and (self.totallength-self.codingObjList[-1].stop) >=minsize :\n",
    "            noncoding.append(\n",
    "                (self.seqid,\n",
    "                 self.codingObjList[-1].stop,\n",
    "                 self.totallength+1,\n",
    "                 f\"{self.seqid}_noncoding_{len(self.codingObjList)+2}\"\n",
    "                 ,str(self.entry.seq[self.codingObjList[-1].stop:self.totallength+1]),\n",
    "                 self.taxid\n",
    "                )\n",
    "            )\n",
    "        return noncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33067588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.bgzf import BgzfWriter, BgzfReader\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm as t\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import multiprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bcf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocesses=60\n",
    "out= \"./240325_reference_genomes/regions/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gbff2Regionslists(origin_file_tuple):\n",
    "    origin, file = origin_file_tuple\n",
    "    response = {\"origin\":origin,\"cdslen\":0,\"noncdslen\":0,\"cdsreg\":[],\"noncdsreg\":[]}\n",
    "    samplecdslength=0\n",
    "    samplenoncdslength=0\n",
    "    global out\n",
    "    outpath = os.path.join(out,f\"{origin}_\"+\".\".join(os.path.basename(file).split(\".\")[:-2]))+\"region.bgzf\"\n",
    "    \n",
    "    try:\n",
    "        with gzip.open(file,\"rt\") as gbff:\n",
    "            with BgzfWriter(outpath,\"w\") as data_out:\n",
    "                response = []\n",
    "                for entry in SeqIO.parse(gbff, \"genbank\"):\n",
    "                    gbffRegions = GBFF_ENTRY(entry)\n",
    "         \n",
    "                    for region in gbffRegions.get_coding():\n",
    "                        length = region[2]-region[1]\n",
    "                        pretell = data_out.tell()\n",
    "                        data_out.write(\"%s\\t%d\\t%d\\t%s\\t%s\\t%s\\n\"%region)\n",
    "                        response.append(f\"{origin}\\tcds\\t{outpath}\\t{length}\\t{pretell}\\n\")\n",
    "                        \n",
    "                    for region in gbffRegions.get_noncoding():\n",
    "                        length = region[2]-region[1]\n",
    "                        pretell = data_out.tell()\n",
    "                        data_out.write(\"%s\\t%d\\t%d\\t%s\\t%s\\t%s\\n\"%region)\n",
    "                        response.append(f\"{origin}\\tncds\\t{outpath}\\t{length}\\t{pretell}\\n\")\n",
    "\n",
    "        return(\"\".join(response))\n",
    "    except gzip.BadGzipFile as e:\n",
    "        print(e)\n",
    "        print(file)\n",
    "    except EOFError as e:\n",
    "        print(e)\n",
    "        print(file)\n",
    "    except Exception as e:\n",
    "        print (\"Other Error:\",e,\"in File\",file)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def region_indexer(dataOutputFile,gbff_tupleList):\n",
    "    total_lengths= dict()\n",
    "    index = dict()\n",
    "    with gzip.open(dataOutputFile,\"wt\") as index_out:\n",
    "        with multiprocessing.Pool(processes=nprocesses) as pool:\n",
    "            for response in t(pool.imap_unordered(gbff2Regionslists,gbff_tupleList),total=len(gbff_tupleList)):\n",
    "                if response:\n",
    "                    index_out.write(response)\n",
    "\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d21065",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbff_files = [(\"mammal_bat\",'240325_reference_genomes/gbff/mammal/bat/GCF_004115265.2_mRhiFer1_v1.p_genomic.gbff.gz'),\n",
    " (\"mammal_pig\",'240325_reference_genomes/gbff/mammal/pig/GCF_000003025.6_Sscrofa11.1_genomic.gbff.gz'),\n",
    " (\"mammal_human\",'240325_reference_genomes/gbff/mammal/human/GCF_000001405.40_GRCh38.p14_genomic.gbff.gz')]\n",
    "\n",
    "gbff_files += [(\"viral\",x) for x in glob(\"240325_reference_genomes/gbff/viral/*.gbff.gz\") ]\n",
    "gbff_files += [(\"bacteria\",x) for x in glob(\"240325_reference_genomes/gbff/bacteria/*.gbff.gz\") ]\n",
    "\n",
    "\n",
    "regionINDEX = region_indexer(\"240325_reference_genomes/240328_regionindex.gz\", gbff_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b25cd",
   "metadata": {},
   "source": [
    "# Reload Index to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(indexfile):\n",
    "    memory_index=dict()\n",
    "    with gzip.open(indexfile,\"rt\") as idxf:\n",
    "        for line in idxf:\n",
    "            origin, cds_status, path,length,datastart= line.strip().split(\"\\t\")\n",
    "            if origin not in memory_index:\n",
    "                memory_index[origin] = {\"cds\":[],\"ncds\":[],\"totalcds\":0,\"totalnoncds\":0}\n",
    "            memory_index[origin][cds_status].append( (int(length),path,int(datastart)))\n",
    "            if cds_status==\"cds\":\n",
    "                memory_index[origin][\"totalcds\"]+=int(length)\n",
    "            else:\n",
    "                memory_index[origin][\"totalnoncds\"]+=int(length)\n",
    "\n",
    "    return memory_index\n",
    "region_index = load_index(\"240325_reference_genomes/240328_regionindex.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede601b",
   "metadata": {},
   "source": [
    "# Get Probability Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probvectors = {}\n",
    "for origin, subdata in region_index.items():\n",
    "    probvectors[origin]={}\n",
    "    for cds_status in [\"cds\",\"ncds\"]:\n",
    "        totallength = subdata[\"totalcds\"] if cds_status ==\"cds\" else subdata[\"totalnoncds\"] \n",
    "        print (totallength)\n",
    "        probvectors[origin][cds_status] = np.asarray([data[0] for data in subdata[cds_status]],dtype=np.double)/totallength\n",
    "print(probvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda71f4",
   "metadata": {},
   "source": [
    "# Define Sample Sizes\n",
    "\n",
    "Training Data: Mammian per cds/noncds --> 166666 Reads\n",
    "Test Data: Mammian per cds/noncds --> 1666 Reads\n",
    "\n",
    "Viral/ Bacterial:\n",
    "Training Data: 500000 Reads per cds/noncds\n",
    "Test Data: 5000 Reads per cds/noncds\n",
    "\n",
    "Draw Random from Kategories without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5920e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mammalsize = (166666,1666)\n",
    "othersize = (500000,5000)\n",
    "readlength = 300\n",
    "\n",
    "\n",
    "# Small set\n",
    "mammalsize = (83334,1666)\n",
    "othersize = (250000,5000)\n",
    "readlength = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35132bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Order in final file\n",
    "\n",
    "order_traindata = [zz for yy in [[(cls,cds_type)]*(mammalsize[0] if cls.startswith(\"mammal\") else othersize[0]) for cls in region_index.keys() for cds_type in [\"cds\",\"ncds\"]] for zz in yy]\n",
    "order_testdata = [zz for yy in [[(cls,cds_type)]*(mammalsize[1] if cls.startswith(\"mammal\") else othersize[1]) for cls in region_index.keys() for cds_type in [\"cds\",\"ncds\"]] for zz in yy]\n",
    "random.seed(42)\n",
    "random.shuffle(order_traindata)\n",
    "random.shuffle(order_testdata)\n",
    "print(order_testdata[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probvectors[origin][cds_status]\n",
    "def randomnesgenerator ( probs, name = None, initlength=600000):\n",
    "    while True:\n",
    "        if name:\n",
    "            print(\"Init:\", name, \"Generator\")\n",
    "        readIndex = np.random.choice(len(probs),initlength, p=probs)\n",
    "        orientation = np.random.choice(2,initlength)\n",
    "        for rI, orientation in zip(readIndex,orientation):\n",
    "            yield rI,orientation\n",
    "        if name:\n",
    "            print(\"Draw new Batch:\",name, \"Generator\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class indexedRead:\n",
    "    def __init__(self,read,orientation,readlength=300):\n",
    "        length, path, datastart = read\n",
    "        self.regionlength = length\n",
    "        self.readlength = readlength\n",
    "        self.orientation = orientation\n",
    "        # Draw Start and Orientation\n",
    "        \n",
    "        self.readstartinregion = 0 if length == readlength else np.random.choice(self.regionlength-readlength,1)[0]\n",
    "        with BgzfReader(path) as data:\n",
    "            data.seek(datastart)\n",
    "            d = data.readline().strip().split(\"\\t\")\n",
    "\n",
    "            self.seqid, rstart,rstop,self.locus, regionSequence,self.taxid = d\n",
    "            self.regionstart = int(rstart)\n",
    "            self.regionstop  = int(rstop)\n",
    "            self.readstartinref =  self.regionstart + self.readstartinregion\n",
    "            \n",
    "\n",
    "            self.readSeq = regionSequence[self.readstartinregion:(self.readstartinregion+readlength)].upper()\n",
    "\n",
    "            self.has_no_ambiguities = not set(self.readSeq).difference({'A','T','C','G'})\n",
    "            \n",
    "            if orientation:\n",
    "                self.readSeq = self.readSeq[::-1].translate({65: 'T', 84: 'A', 71: 'C', 67: 'G'})\n",
    "            self.readblocks = \" \".join([self.readSeq[index:index+6] for index in range(readlength-5)])\n",
    "\n",
    "def readtuple2line(index,origin,cds_status,readlength = 300):\n",
    "    # Draw single read\n",
    "    timeout = 10000\n",
    "\n",
    "    # Avoid Infinit Loop\n",
    "    for i in range(timeout):\n",
    "        singleReadFromIndexPOS, reversedRead =next(randgen[origin][cds_status])\n",
    "\n",
    "        # Single Read Attributes\n",
    "        read = index[origin][cds_status][singleReadFromIndexPOS]\n",
    "        \n",
    "        # Define DNA-BERT Label\n",
    "        label = 1 if cds_status == \"cds\" else 0\n",
    "        readentry = indexedRead(read,reversedRead)\n",
    "        if readentry.has_no_ambiguities:\n",
    "            return f\"{readentry.readblocks}\\t{label}\\t{origin}\\t{readentry.seqid}\\t{readentry.taxid}\\t{readentry.locus}\\t{readentry.readstartinref}\\t{readentry.regionstart}\\t{readentry.regionstop}\\t{reversedRead}\\n\"\n",
    "            #return f\"{readentry.readSeq}\\t{label}\\t{origin}\\t{readentry.seqid}\\t{readentry.taxid}\\t{readentry.locus}\\t{readentry.readstartinref}\\t{readentry.regionstart}\\t{readentry.regionstop}\\t{reversedRead}\\n\"\n",
    "\n",
    "    print(origin,cds_status,\"timeout\")\n",
    "    assert False\n",
    "\n",
    "def readsFromIndex2Tsv(index,readcats,outputfile):\n",
    "    with gzip.open(outputfile,\"wt\") as output:\n",
    "        output.write(\"Sequence\\tLabel\\tOrigin\\tReferenceID\\ttaxid\\tLocus\\tStartInReference\\tRegionStartinRef\\tRegionStopinRef\\tReversed\\n\")\n",
    "        for readclass in t(readcats):\n",
    "            output.write(readtuple2line(index,*readclass))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ccecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "## init\n",
    "randgen = {cls: {cds_type:randomnesgenerator(probs,f\"{cls} {cds_type}\") for cds_type, probs in v1.items()} for cls,v1 in probvectors.items() }\n",
    "print(randgen)\n",
    "    \n",
    "readsFromIndex2Tsv(region_index,order_testdata,\"240325_reference_genomes/traindata/dev_Smallk.tsv.gz\")\n",
    "readsFromIndex2Tsv(region_index,order_traindata,\"240325_reference_genomes/traindata/train_Smallk.tsv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe09143",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "## init\n",
    "randgen = {cls: {cds_type:randomnesgenerator(probs,f\"{cls} {cds_type}\") for cds_type, probs in v1.items()} for cls,v1 in probvectors.items() }\n",
    "print(randgen)\n",
    "    \n",
    "readsFromIndex2Tsv(region_index,order_testdata,\"240325_reference_genomes/traindata/dev.tsv.gz\")\n",
    "readsFromIndex2Tsv(region_index,order_traindata,\"240325_reference_genomes/traindata/train.tsv.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
